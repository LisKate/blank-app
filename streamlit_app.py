
import streamlit as st
import pandas as pd
import requests
import json
import time
from openai import OpenAI
from io import BytesIO
import anthropic
from yandex_cloud_ml_sdk import YCloudML
from mistralai import Mistral
from time import sleep
import xlsxwriter, openpyxl
import os

os.environ['nebius_api'] = st.secrets['nebius_api']
os.environ['antropic_api'] = st.secrets['antropic_api']
os.environ['yandex_api'] = st.secrets['yandex_api']
os.environ['mistral_api'] = st.secrets['mistral_api']
os.environ['openrouter_api'] = st.secrets['openrouter_api']
os.environ['folder_id1'] = st.secrets['folder_id1']

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–æ–≤
def init_clients():
    return {
        "anthropic": anthropic.Client(api_key=os.environ['antropic_api']),
        "nebius": OpenAI(
            base_url="https://api.studio.nebius.ai/v1/",

            api_key=os.environ['nebius_api']
        ),
        "yandex": YCloudML(
            folder_id=os.environ['folder_id1'],
            auth=os.environ['yandex_api']
        ),
        "mistral": Mistral(api_key=os.environ['mistral_api'] ),
        "openrouter": OpenAI(
            base_url="https://openrouter.ai/api/v1",
            api_key=os.environ['openrouter_api']
        )
    }

clients = init_clients()

# –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
MODELS = {
    "Claude3.5": {"provider": "anthropic", "model": "claude-3-sonnet-20240229"},
    "Claude3.7": {"provider": "anthropic", "model": "claude-3-haiku-20240307"},
    "DeepSeek-V3": {"provider": "nebius", "model": "deepseek-ai/DeepSeek-V3"},
    "DeepSeek-R1": {"provider": "nebius", "model": "deepseek-ai/DeepSeek-R1"},
    "Grok-2": {"provider": "openrouter", "model": "x-ai/grok-2-1212"},
    "YandexGPT5": {"provider": "yandex", "model": "yandexgpt"},
    "Gemini": {"provider": "openrouter", "model": "google/gemini-2.0-flash-001"},
    "Llama-3-70B": {"provider": "openrouter", "model": "meta-llama/Meta-Llama-3.1-70B-Instruct"},
    "Hermes": {"provider": "openrouter", "model": "NousResearch/Hermes-3-Llama-3.1-70B"},
    "Qwen": {"provider": "openrouter", "model": "Qwen/Qwen2.5-72B-Instruct"},
    "Mistral-Large": {"provider": "mistral", "model": "mistral-large-latest"}
}

# ======================================
# –í–∫–ª–∞–¥–∫–∞ 1: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
# ======================================
def dataset_testing_tab():
    st.header("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LLM –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
    @st.cache_data
    def load_dataset(dataset_name):
        try:
            if dataset_name == "CISM":
                return pd.read_excel("cism_eng_dataset.xlsx")
            elif dataset_name == "CISSP":
                return pd.read_excel("cissp_eng_dataset.xlsx")
            elif dataset_name == "CISSP5":
                return pd.read_excel("cissp_eng_5.xlsx")
            elif dataset_name == "—Ç–µ—Å—Ç":
                return pd.read_excel("—Ç–µ—Å—Ç.xlsx")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
            return None

    dataset_name = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç", ["CISM", "CISSP", "CISSP5","—Ç–µ—Å—Ç"])
    dataset = load_dataset(dataset_name)

    if dataset is not None:
        st.subheader("–ü–µ—Ä–≤—ã–µ 5 –≤–æ–ø—Ä–æ—Å–æ–≤:")
        st.dataframe(dataset.head())

        model_choice = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è",
            list(MODELS.keys())
        )

        if st.button("üöÄ –ù–∞—á–∞—Ç—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ"):
            test_model(model_choice, dataset)

def test_model(model_name, dataset):
    model_info = MODELS[model_name]
    responses = []
    df = dataset.copy()

    with st.spinner(f"–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ {model_name}..."):
        progress_bar = st.progress(0)
        total_questions = len(df)

        for i, row in df.iterrows():
            answer = "ERROR"
            try:
                system_prompt = str(row['System_prompt'])
                question = str(row['Question'])

                if model_info["provider"] == "anthropic":
                    response = clients["anthropic"].messages.create(
                        model=model_info["model"],
                        messages=[{"role": "user", "content": f"{system_prompt}\n{question}"}],
                        max_tokens=50
                    )
                    answer = response.content[0].text.strip()

                elif model_info["provider"] == "nebius":
                    response = clients["nebius"].chat.completions.create(
                        model=model_info["model"],
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": question},
                        ],
                        max_tokens=500,
                        temperature=0.3,
                    )
                    answer = response.choices[0].message.content.strip()

                elif model_info["provider"] == "openrouter":
                    response = clients["openrouter"].chat.completions.create(
                        model=model_info["model"],
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": question},
                        ],
                        max_tokens=500,
                        temperature=0.7
                    )
                    answer = response.choices[0].message.content.strip()
                    sleep(1)

                elif model_info["provider"] == "yandex":
                    result = clients["yandex"].models.completions("yandexgpt").run(
                        messages=[
                            {"role": "system", "text": system_prompt},
                            {"role": "user", "text": question}
                        ]
                    )
                    if hasattr(result, 'alternatives') and result.alternatives:
                        answer = result.alternatives[0].message.text.strip()

                elif model_info["provider"] == "mistral":
                    response = clients["mistral"].chat(
                        model=model_info["model"],
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": question}
                        ],
                        temperature=0.7
                    )
                    answer = response.choices[0].message.content.strip()

                responses.append(answer)
                progress_bar.progress((i + 1) / total_questions)
                time.sleep(0.5)

            except Exception as e:
                st.error(f"–û—à–∏–±–∫–∞ –≤ –≤–æ–ø—Ä–æ—Å–µ {i+1}: {str(e)}")
                responses.append(answer)
                progress_bar.progress((i + 1) / total_questions)

    df[model_name] = responses
    st.success("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    st.dataframe(df)

    # –ö–Ω–æ–ø–∫–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
    col1, col2 = st.columns(2)
    with col1:
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å CSV",
            data=df.to_csv(index=False).encode('utf-8'),
            file_name=f'results_{model_name}.csv',
            mime='text/csv'
        )
    with col2:
        output = BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df.to_excel(writer, index=False)
        st.download_button(
            label="üì• –°–∫–∞—á–∞—Ç—å Excel",
            data=output.getvalue(),
            file_name=f'results_{model_name}.xlsx',
            mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
        )

# ======================================
# –í–∫–ª–∞–¥–∫–∞ 2: –ß–∞—Ç —Å LLM
# ======================================
def chat_tab():
    st.header("–ß–∞—Ç —Å LLM")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
    model_choice = st.selectbox(
        "–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞",
        list(MODELS.keys())
    )

    # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
    system_prompt = st.text_area(
        "–†–æ–ª—å —Å–∏—Å—Ç–µ–º—ã:",
        "You are a helpful AI assistant. Provide concise and accurate answers.",
        height=100
    )

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞
    if prompt := st.chat_input("–í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å..."):
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        messages = [{"role": "system", "content": system_prompt}] + st.session_state.messages

        with st.chat_message("assistant"):
            with st.spinner("–î—É–º–∞—é..."):
                try:
                    model_info = MODELS[model_choice]

                    if model_info["provider"] == "anthropic":
                        response = clients["anthropic"].messages.create(
                            model=model_info["model"],
                            messages=messages,
                            max_tokens=1000
                        )
                        answer = response.content[0].text

                    elif model_info["provider"] == "nebius":
                        response = clients["nebius"].chat.completions.create(
                            model=model_info["model"],
                            messages=messages,
                            max_tokens=1000
                        )
                        answer = response.choices[0].message.content

                    elif model_info["provider"] == "openrouter":
                        response = clients["openrouter"].chat.completions.create(
                            model=model_info["model"],
                            messages=messages,
                            max_tokens=1000
                        )
                        answer = response.choices[0].message.content

                    elif model_info["provider"] == "yandex":
                        result = clients["yandex"].models.completions("yandexgpt").run(
                            messages=messages
                        )
                        answer = result.alternatives[0].message.text

                    elif model_info["provider"] == "mistral":
                        response = clients["mistral"].chat(
                            model=model_info["model"],
                            messages=messages,
                            temperature=0.7
                        )
                        answer = response.choices[0].message.content

                    st.markdown(answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer})

                except Exception as e:
                    st.error(f"–û—à–∏–±–∫–∞: {str(e)}")

# ======================================
# –ì–ª–∞–≤–Ω–æ–µ –º–µ–Ω—é
# ======================================
st.sidebar.title("LLM Sandbox")
tab = st.sidebar.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º", ["–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤", "–ß–∞—Ç —Å LLM"])

if tab == "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤":
    dataset_testing_tab()
else:
    chat_tab()
